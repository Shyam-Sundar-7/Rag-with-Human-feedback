{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file has been saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csv_files(folder_path):\n",
    "    # Get a list of all CSV files in the specified folder\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty DataFrame to store merged data\n",
    "    merged_df = pd.DataFrame(columns=[\"question\", \"context\", \"score\"])\n",
    "\n",
    "    # Iterate through each CSV file\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "\n",
    "        # Extract required columns if they exist in the DataFrame\n",
    "        if \"question\" in df.columns and \"context\" in df.columns and \"score\" in df.columns:\n",
    "            # Append only the required columns to the merged DataFrame\n",
    "            merged_df = pd.concat([merged_df, df[[\"question\", \"context\", \"score\"]]], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Skipping {file} as it does not contain all required columns.\")\n",
    "\n",
    "    # Write the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(\"merged_training_data.csv\", index=False)\n",
    "    print(\"Merged CSV file has been saved.\")\n",
    "\n",
    "# Specify the folder path where CSV files are located\n",
    "training_folder_path = \"training\"\n",
    "\n",
    "# Call the function to merge CSV files\n",
    "merge_csv_files(training_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\Rag-with-Human-feedback\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0009],\n",
      "        [0.0231]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/qnli-distilroberta-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/qnli-distilroberta-base')\n",
    "\n",
    "features = tokenizer([('How many people live in Berlin?', 'What is the size of New York?'),('Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.')],  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = torch.nn.functional.sigmoid(model(**features).logits)\n",
    "    print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "class CustomRobertaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomRobertaClassifier, self).__init__()\n",
    "        self.roberta = RobertaForSequenceClassification.from_pretrained(\"cross-encoder/qnli-distilroberta-base\")\n",
    "        # self.tokenizer = RobertaTokenizer.from_pretrained(\"cross-encoder/qnli-distilroberta-base\")\n",
    "        \n",
    "        # Freeze the RoBERTa model weights\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Make the classifier part trainable\n",
    "        for param in self.roberta.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Modify the output layer to have num_labels output neurons\n",
    "        self.roberta.classifier.out_proj = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids,attention_mask):\n",
    "        # Forward pass through RoBERTa model\n",
    "        \n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "Custom_model=CustomRobertaClassifier(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomRobertaClassifier(\n",
       "  (roberta): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Custom_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\",\"Parameters\"])\n",
    "    total_params = 0\n",
    "    total_trainganle_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        params = parameter.numel()\n",
    "        total_params+=params\n",
    "        table.add_row([name, params])\n",
    "        if not parameter.requires_grad: \n",
    "            continue\n",
    "        else:\n",
    "            total_trainganle_params+=params\n",
    "        \n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_trainganle_params}\")\n",
    "    print(f\"Total Params: {total_params}\")\n",
    "    print(f\"Ratio : {total_trainganle_params/total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+------------+\n",
      "|                          Modules                          | Parameters |\n",
      "+-----------------------------------------------------------+------------+\n",
      "|         roberta.embeddings.word_embeddings.weight         |  38603520  |\n",
      "|       roberta.embeddings.position_embeddings.weight       |   394752   |\n",
      "|      roberta.embeddings.token_type_embeddings.weight      |    768     |\n",
      "|            roberta.embeddings.LayerNorm.weight            |    768     |\n",
      "|             roberta.embeddings.LayerNorm.bias             |    768     |\n",
      "|    roberta.encoder.layer.0.attention.self.query.weight    |   589824   |\n",
      "|     roberta.encoder.layer.0.attention.self.query.bias     |    768     |\n",
      "|     roberta.encoder.layer.0.attention.self.key.weight     |   589824   |\n",
      "|      roberta.encoder.layer.0.attention.self.key.bias      |    768     |\n",
      "|    roberta.encoder.layer.0.attention.self.value.weight    |   589824   |\n",
      "|     roberta.encoder.layer.0.attention.self.value.bias     |    768     |\n",
      "|   roberta.encoder.layer.0.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.encoder.layer.0.attention.output.dense.bias    |    768     |\n",
      "| roberta.encoder.layer.0.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.encoder.layer.0.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.encoder.layer.0.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.encoder.layer.0.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.encoder.layer.0.output.dense.weight        |  2359296   |\n",
      "|         roberta.encoder.layer.0.output.dense.bias         |    768     |\n",
      "|      roberta.encoder.layer.0.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.encoder.layer.0.output.LayerNorm.bias       |    768     |\n",
      "|    roberta.encoder.layer.1.attention.self.query.weight    |   589824   |\n",
      "|     roberta.encoder.layer.1.attention.self.query.bias     |    768     |\n",
      "|     roberta.encoder.layer.1.attention.self.key.weight     |   589824   |\n",
      "|      roberta.encoder.layer.1.attention.self.key.bias      |    768     |\n",
      "|    roberta.encoder.layer.1.attention.self.value.weight    |   589824   |\n",
      "|     roberta.encoder.layer.1.attention.self.value.bias     |    768     |\n",
      "|   roberta.encoder.layer.1.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.encoder.layer.1.attention.output.dense.bias    |    768     |\n",
      "| roberta.encoder.layer.1.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.encoder.layer.1.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.encoder.layer.1.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.encoder.layer.1.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.encoder.layer.1.output.dense.weight        |  2359296   |\n",
      "|         roberta.encoder.layer.1.output.dense.bias         |    768     |\n",
      "|      roberta.encoder.layer.1.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.encoder.layer.1.output.LayerNorm.bias       |    768     |\n",
      "|    roberta.encoder.layer.2.attention.self.query.weight    |   589824   |\n",
      "|     roberta.encoder.layer.2.attention.self.query.bias     |    768     |\n",
      "|     roberta.encoder.layer.2.attention.self.key.weight     |   589824   |\n",
      "|      roberta.encoder.layer.2.attention.self.key.bias      |    768     |\n",
      "|    roberta.encoder.layer.2.attention.self.value.weight    |   589824   |\n",
      "|     roberta.encoder.layer.2.attention.self.value.bias     |    768     |\n",
      "|   roberta.encoder.layer.2.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.encoder.layer.2.attention.output.dense.bias    |    768     |\n",
      "| roberta.encoder.layer.2.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.encoder.layer.2.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.encoder.layer.2.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.encoder.layer.2.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.encoder.layer.2.output.dense.weight        |  2359296   |\n",
      "|         roberta.encoder.layer.2.output.dense.bias         |    768     |\n",
      "|      roberta.encoder.layer.2.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.encoder.layer.2.output.LayerNorm.bias       |    768     |\n",
      "|    roberta.encoder.layer.3.attention.self.query.weight    |   589824   |\n",
      "|     roberta.encoder.layer.3.attention.self.query.bias     |    768     |\n",
      "|     roberta.encoder.layer.3.attention.self.key.weight     |   589824   |\n",
      "|      roberta.encoder.layer.3.attention.self.key.bias      |    768     |\n",
      "|    roberta.encoder.layer.3.attention.self.value.weight    |   589824   |\n",
      "|     roberta.encoder.layer.3.attention.self.value.bias     |    768     |\n",
      "|   roberta.encoder.layer.3.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.encoder.layer.3.attention.output.dense.bias    |    768     |\n",
      "| roberta.encoder.layer.3.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.encoder.layer.3.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.encoder.layer.3.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.encoder.layer.3.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.encoder.layer.3.output.dense.weight        |  2359296   |\n",
      "|         roberta.encoder.layer.3.output.dense.bias         |    768     |\n",
      "|      roberta.encoder.layer.3.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.encoder.layer.3.output.LayerNorm.bias       |    768     |\n",
      "|    roberta.encoder.layer.4.attention.self.query.weight    |   589824   |\n",
      "|     roberta.encoder.layer.4.attention.self.query.bias     |    768     |\n",
      "|     roberta.encoder.layer.4.attention.self.key.weight     |   589824   |\n",
      "|      roberta.encoder.layer.4.attention.self.key.bias      |    768     |\n",
      "|    roberta.encoder.layer.4.attention.self.value.weight    |   589824   |\n",
      "|     roberta.encoder.layer.4.attention.self.value.bias     |    768     |\n",
      "|   roberta.encoder.layer.4.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.encoder.layer.4.attention.output.dense.bias    |    768     |\n",
      "| roberta.encoder.layer.4.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.encoder.layer.4.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.encoder.layer.4.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.encoder.layer.4.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.encoder.layer.4.output.dense.weight        |  2359296   |\n",
      "|         roberta.encoder.layer.4.output.dense.bias         |    768     |\n",
      "|      roberta.encoder.layer.4.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.encoder.layer.4.output.LayerNorm.bias       |    768     |\n",
      "|    roberta.encoder.layer.5.attention.self.query.weight    |   589824   |\n",
      "|     roberta.encoder.layer.5.attention.self.query.bias     |    768     |\n",
      "|     roberta.encoder.layer.5.attention.self.key.weight     |   589824   |\n",
      "|      roberta.encoder.layer.5.attention.self.key.bias      |    768     |\n",
      "|    roberta.encoder.layer.5.attention.self.value.weight    |   589824   |\n",
      "|     roberta.encoder.layer.5.attention.self.value.bias     |    768     |\n",
      "|   roberta.encoder.layer.5.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.encoder.layer.5.attention.output.dense.bias    |    768     |\n",
      "| roberta.encoder.layer.5.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.encoder.layer.5.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.encoder.layer.5.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.encoder.layer.5.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.encoder.layer.5.output.dense.weight        |  2359296   |\n",
      "|         roberta.encoder.layer.5.output.dense.bias         |    768     |\n",
      "|      roberta.encoder.layer.5.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.encoder.layer.5.output.LayerNorm.bias       |    768     |\n",
      "|                  classifier.dense.weight                  |   589824   |\n",
      "|                   classifier.dense.bias                   |    768     |\n",
      "|                 classifier.out_proj.weight                |    768     |\n",
      "|                  classifier.out_proj.bias                 |     1      |\n",
      "+-----------------------------------------------------------+------------+\n",
      "Total Trainable Params: 82119169\n",
      "Total Params: 82119169\n",
      "Ratio : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82119169"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+------------+\n",
      "|                              Modules                              | Parameters |\n",
      "+-------------------------------------------------------------------+------------+\n",
      "|         roberta.roberta.embeddings.word_embeddings.weight         |  38603520  |\n",
      "|       roberta.roberta.embeddings.position_embeddings.weight       |   394752   |\n",
      "|      roberta.roberta.embeddings.token_type_embeddings.weight      |    768     |\n",
      "|            roberta.roberta.embeddings.LayerNorm.weight            |    768     |\n",
      "|             roberta.roberta.embeddings.LayerNorm.bias             |    768     |\n",
      "|    roberta.roberta.encoder.layer.0.attention.self.query.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.0.attention.self.query.bias     |    768     |\n",
      "|     roberta.roberta.encoder.layer.0.attention.self.key.weight     |   589824   |\n",
      "|      roberta.roberta.encoder.layer.0.attention.self.key.bias      |    768     |\n",
      "|    roberta.roberta.encoder.layer.0.attention.self.value.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.0.attention.self.value.bias     |    768     |\n",
      "|   roberta.roberta.encoder.layer.0.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.roberta.encoder.layer.0.attention.output.dense.bias    |    768     |\n",
      "| roberta.roberta.encoder.layer.0.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.roberta.encoder.layer.0.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.roberta.encoder.layer.0.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.roberta.encoder.layer.0.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.roberta.encoder.layer.0.output.dense.weight        |  2359296   |\n",
      "|         roberta.roberta.encoder.layer.0.output.dense.bias         |    768     |\n",
      "|      roberta.roberta.encoder.layer.0.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.roberta.encoder.layer.0.output.LayerNorm.bias       |    768     |\n",
      "|    roberta.roberta.encoder.layer.1.attention.self.query.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.1.attention.self.query.bias     |    768     |\n",
      "|     roberta.roberta.encoder.layer.1.attention.self.key.weight     |   589824   |\n",
      "|      roberta.roberta.encoder.layer.1.attention.self.key.bias      |    768     |\n",
      "|    roberta.roberta.encoder.layer.1.attention.self.value.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.1.attention.self.value.bias     |    768     |\n",
      "|   roberta.roberta.encoder.layer.1.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.roberta.encoder.layer.1.attention.output.dense.bias    |    768     |\n",
      "| roberta.roberta.encoder.layer.1.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.roberta.encoder.layer.1.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.roberta.encoder.layer.1.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.roberta.encoder.layer.1.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.roberta.encoder.layer.1.output.dense.weight        |  2359296   |\n",
      "|         roberta.roberta.encoder.layer.1.output.dense.bias         |    768     |\n",
      "|      roberta.roberta.encoder.layer.1.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.roberta.encoder.layer.1.output.LayerNorm.bias       |    768     |\n",
      "|    roberta.roberta.encoder.layer.2.attention.self.query.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.2.attention.self.query.bias     |    768     |\n",
      "|     roberta.roberta.encoder.layer.2.attention.self.key.weight     |   589824   |\n",
      "|      roberta.roberta.encoder.layer.2.attention.self.key.bias      |    768     |\n",
      "|    roberta.roberta.encoder.layer.2.attention.self.value.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.2.attention.self.value.bias     |    768     |\n",
      "|   roberta.roberta.encoder.layer.2.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.roberta.encoder.layer.2.attention.output.dense.bias    |    768     |\n",
      "| roberta.roberta.encoder.layer.2.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.roberta.encoder.layer.2.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.roberta.encoder.layer.2.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.roberta.encoder.layer.2.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.roberta.encoder.layer.2.output.dense.weight        |  2359296   |\n",
      "|         roberta.roberta.encoder.layer.2.output.dense.bias         |    768     |\n",
      "|      roberta.roberta.encoder.layer.2.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.roberta.encoder.layer.2.output.LayerNorm.bias       |    768     |\n",
      "|    roberta.roberta.encoder.layer.3.attention.self.query.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.3.attention.self.query.bias     |    768     |\n",
      "|     roberta.roberta.encoder.layer.3.attention.self.key.weight     |   589824   |\n",
      "|      roberta.roberta.encoder.layer.3.attention.self.key.bias      |    768     |\n",
      "|    roberta.roberta.encoder.layer.3.attention.self.value.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.3.attention.self.value.bias     |    768     |\n",
      "|   roberta.roberta.encoder.layer.3.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.roberta.encoder.layer.3.attention.output.dense.bias    |    768     |\n",
      "| roberta.roberta.encoder.layer.3.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.roberta.encoder.layer.3.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.roberta.encoder.layer.3.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.roberta.encoder.layer.3.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.roberta.encoder.layer.3.output.dense.weight        |  2359296   |\n",
      "|         roberta.roberta.encoder.layer.3.output.dense.bias         |    768     |\n",
      "|      roberta.roberta.encoder.layer.3.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.roberta.encoder.layer.3.output.LayerNorm.bias       |    768     |\n",
      "|    roberta.roberta.encoder.layer.4.attention.self.query.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.4.attention.self.query.bias     |    768     |\n",
      "|     roberta.roberta.encoder.layer.4.attention.self.key.weight     |   589824   |\n",
      "|      roberta.roberta.encoder.layer.4.attention.self.key.bias      |    768     |\n",
      "|    roberta.roberta.encoder.layer.4.attention.self.value.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.4.attention.self.value.bias     |    768     |\n",
      "|   roberta.roberta.encoder.layer.4.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.roberta.encoder.layer.4.attention.output.dense.bias    |    768     |\n",
      "| roberta.roberta.encoder.layer.4.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.roberta.encoder.layer.4.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.roberta.encoder.layer.4.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.roberta.encoder.layer.4.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.roberta.encoder.layer.4.output.dense.weight        |  2359296   |\n",
      "|         roberta.roberta.encoder.layer.4.output.dense.bias         |    768     |\n",
      "|      roberta.roberta.encoder.layer.4.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.roberta.encoder.layer.4.output.LayerNorm.bias       |    768     |\n",
      "|    roberta.roberta.encoder.layer.5.attention.self.query.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.5.attention.self.query.bias     |    768     |\n",
      "|     roberta.roberta.encoder.layer.5.attention.self.key.weight     |   589824   |\n",
      "|      roberta.roberta.encoder.layer.5.attention.self.key.bias      |    768     |\n",
      "|    roberta.roberta.encoder.layer.5.attention.self.value.weight    |   589824   |\n",
      "|     roberta.roberta.encoder.layer.5.attention.self.value.bias     |    768     |\n",
      "|   roberta.roberta.encoder.layer.5.attention.output.dense.weight   |   589824   |\n",
      "|    roberta.roberta.encoder.layer.5.attention.output.dense.bias    |    768     |\n",
      "| roberta.roberta.encoder.layer.5.attention.output.LayerNorm.weight |    768     |\n",
      "|  roberta.roberta.encoder.layer.5.attention.output.LayerNorm.bias  |    768     |\n",
      "|     roberta.roberta.encoder.layer.5.intermediate.dense.weight     |  2359296   |\n",
      "|      roberta.roberta.encoder.layer.5.intermediate.dense.bias      |    3072    |\n",
      "|        roberta.roberta.encoder.layer.5.output.dense.weight        |  2359296   |\n",
      "|         roberta.roberta.encoder.layer.5.output.dense.bias         |    768     |\n",
      "|      roberta.roberta.encoder.layer.5.output.LayerNorm.weight      |    768     |\n",
      "|       roberta.roberta.encoder.layer.5.output.LayerNorm.bias       |    768     |\n",
      "|                  roberta.classifier.dense.weight                  |   589824   |\n",
      "|                   roberta.classifier.dense.bias                   |    768     |\n",
      "|                 roberta.classifier.out_proj.weight                |    768     |\n",
      "|                  roberta.classifier.out_proj.bias                 |     1      |\n",
      "+-------------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 591361\n",
      "Total Params: 82119169\n",
      "Ratio : 0.0072012540701672225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82119169"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model=Custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7201254070167222"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "591361/82119169 *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset=pd.read_csv(\"merged_training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is   Model Registry</td>\n",
       "      <td>models. We will now move on to the other criti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is   Model Registry</td>\n",
       "      <td>relevant elements of the context of your syste...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is   Model Registry</td>\n",
       "      <td>model format abstraction and Model Registry  c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is   Model Registry</td>\n",
       "      <td>models: sklearn, XGBoost, TensorFlow, H20, fas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is   Model Registry</td>\n",
       "      <td>Introducing Model Registry     95\\r\\nIn the ML...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   question  \\\n",
       "0  what is   Model Registry   \n",
       "1  what is   Model Registry   \n",
       "2  what is   Model Registry   \n",
       "3  what is   Model Registry   \n",
       "4  what is   Model Registry   \n",
       "\n",
       "                                             context  score  \n",
       "0  models. We will now move on to the other criti...      0  \n",
       "1  relevant elements of the context of your syste...      0  \n",
       "2  model format abstraction and Model Registry  c...      0  \n",
       "3  models: sklearn, XGBoost, TensorFlow, H20, fas...      0  \n",
       "4  Introducing Model Registry     95\\r\\nIn the ML...      0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\Rag-with-Human-feedback\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch shape: torch.Size([39, 1024])\n",
      "attention batch shape: torch.Size([39, 1024])\n",
      "Label: torch.Size([39])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.img_labels = pd.read_csv(\"merged_training_data.csv\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = list(self.img_labels.iloc[idx, 0:2])\n",
    "        label = torch.tensor(self.img_labels.iloc[idx, 2], dtype=torch.int)\n",
    "\n",
    "        # Tokenize and pad the sequences\n",
    "        token = self.tokenizer(text, padding='max_length', max_length=self.max_length, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # Flatten the tokens\n",
    "        inputs_ids=token['input_ids']\n",
    "        attention_mask = token['attention_mask']\n",
    "        # print(inputs_ids.shape)\n",
    "        inputs_ids = inputs_ids.flatten()\n",
    "        \n",
    "        attention_mask = attention_mask.flatten()\n",
    "        return inputs_ids,attention_mask, label\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"cross-encoder/qnli-distilroberta-base\")\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataset = CustomImageDataset(tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Display image and label.\n",
    "input_ids,attention_mask, train_labels = next(iter(train_dataloader))\n",
    "print(f\"input batch shape: {input_ids.shape}\")\n",
    "print(f\"attention batch shape: {attention_mask.shape}\")\n",
    "print(f\"Label: {train_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1024) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [39, 1024].  Tensor sizes: [1, 514]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mCustom_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Rag-with-Human-feedback\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Rag-with-Human-feedback\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[57], line 25\u001b[0m, in \u001b[0;36mCustomRobertaClassifier.forward\u001b[1;34m(self, input_ids, attention_masks)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids,attention_masks):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Forward pass through RoBERTa model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_masks\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Rag-with-Human-feedback\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Rag-with-Human-feedback\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Rag-with-Human-feedback\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1191\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1191\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1202\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1203\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Rag-with-Human-feedback\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Rag-with-Human-feedback\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\Rag-with-Human-feedback\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:794\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    793\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[1;32m--> 794\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    795\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (1024) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [39, 1024].  Tensor sizes: [1, 514]"
     ]
    }
   ],
   "source": [
    "Custom_model(input_ids=input_ids,attention_masks=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model3 = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
